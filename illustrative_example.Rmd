---
title: "Illustrative Example"
author: "Alex Carriero"
output: html_document
---

# Set Up
```{r, message = F, warning = F}
# libraries
library(tidyverse)
library(xgboost)
library(kernelshap)
library(treeshap)
library(shapviz)
```

# Data Generating Mechanism 
```{r}
# data generation 

# seed
set.seed(119)

# population sample size
n = 200000

# generate variable A and variable X
a <- rbinom(n, size = 1, prob = 0.2) 
x <- 3*a + rnorm(n, mean = 0, sd = 1) # A causes X

# generate variable B
b <- runif(n, min = 0, max = 1)

# generate variable Z and variable C
z <- rnorm(n, mean = 0, sd = 1)
c <- ifelse(z > 0.67, 1, 0) # Z causes C

# generate risk of kidney disease
lp <- x + 1.5*b + 2*z + rnorm(n, mean = 0, sd = 0.25) - 1.3 # linear predictor
p  <- exp(lp) / (1 + exp(lp)) # risk of outcome

# visualize true risk in population
hist(p)

# generate outcome variable 
out <- rbinom(n, size = 1, prob = p) %>% as.factor()

# generate variable Y 
y <- rbinom(n, size = 1, prob = ifelse( (p + b) > 1, 1, 0)) # Y is caused by outcome and variable B

df <- tibble(
  "Smoking Status (variable A)" = a, 
  "Hypertension (variable X)" = x, 
  "Age (variable B)" = b, 
  "Insulin Prescription (variable C)" = c, 
  Y = y,
  out = out
)

# filter for only hospitalized patients (filter population based on Y)
df <- df %>% filter (Y == 1)

# filter for model development sample size
df <- sample_n(df, 20000, replace = FALSE)  %>% select(-Y)

# show data frame 
# head(df)
```

# XGBoost 

```{r, message = F}
# xgboost model
# optimized with logloss

mod <- xgboost(df %>% select(-out) %>% as.matrix(), df$out %>% as.numeric() -1, 
               max.depth = 2, eta = 0.3, nround = 50, objective = "binary:logistic", verbose = F)
```


```{r}
# treeSHAP
bg_x <- df[sample(nrow(df), 5000), ]
unified_model <- xgboost.unify(mod, data = bg_x)

sv_tree <- treeshap(unified_model, df %>% select(-out), verbose = FALSE)
tshap = shapviz(sv_tree)

sv_importance(tshap, kind = "bee") + theme_minimal()
sv_importance(tshap, show_numbers = TRUE) + theme_minimal()
```

# Logistic Regression 
```{r, message = F, warning = F, comment = "   "}
# fit model 
mod <- glm(out ~., family = "binomial", data = df)
summary(mod)
```

```{r, message = F, warning = F, comment = "   ", results = "hide"}
# kernel shap 
features <- colnames(df %>% select(-out))
x_train  <- df %>% select(-out)
bg_x <- df[sample(nrow(df), 500), ]

shap <- kernelshap(mod, x_train, bg_X = bg_x)
plot <- shapviz(shap)
```

```{r}
# visualize results
sv_importance(plot, kind = "bee") + theme_minimal()
sv_importance(plot, show_numbers = TRUE) + theme_minimal()
```

