---
title: "Explainable AI in Healthcare: to Explain, to Predict, or to Describe?"
output: html_document
---

This document contains the code that generates our illustrative example. \
\

If you have any questions or feedback please don't hesitate to contact Alex Carriero (corresponding author): a.j.carriero@umcutrecht.nl

## Set Up
```{r, message = F, warning = F}
# libraries
library(tidyverse)
library(xgboost)
library(kernelshap)
library(treeshap)
library(shapviz)
library(pROC)
```

## Data Generating Mechanism 
```{r}
# write function to generate data 
generate_data <- function(sample_size){
  
  # set super population sample size
  n = 500000
  
  # generate variable A and variable X (such that A causes X)
  a <- rbinom(n, size = 1, prob = 0.2) 
  x <- 3*a + rnorm(n, mean = 0, sd = 1) 
  
  # generate variable B
  b <- runif(n, min = 0, max = 1)
  
  # generate variable Z and variable C (such that Z causes C)
  z <- rnorm(n, mean = 0, sd = 1)
  c <- ifelse(z > 0.67, 1, 0) 
  
  # generate risk of kidney disease
  lp <- x + 1.5*b + 2*z + rnorm(n, mean = 0, sd = 0.25) - 1.3   # linear predictor
  p  <- exp(lp) / (1 + exp(lp))                                 # probability of outcome
  
  # generate outcome variable 
  out <- rbinom(n, size = 1, prob = p) %>% as.factor()
  
  # generate variable Y (such that Y is caused by both the outcome and B)
  y <- rbinom(n, size = 1, prob = ifelse( (p + b) > 1, 1, 0)) 
  
  df <- tibble(
  "Smoking Status (variable A)" = a, 
  "Hypertension (variable X)" = x, 
  "Age (variable B)" = b, 
  "Insulin Prescription (variable C)" = c, 
  Y = y,
  out = out
  )

  # filter for only hospitalized patients (filter population based on Y) to introduce collider bias
  df <- df %>% filter (Y == 1)
  
  # filter for model development sample size
  df <- sample_n(df, sample_size, replace = FALSE)  %>% select(-Y)
  
  return(df)
}

# seed
set.seed(119)

# training data
train_data <- generate_data(sample_size = 20000)

# validation data 
valid_data <- generate_data(sample_size = 100000) 

# note: we make validation data large to ensure unbiased and precise estimates of model performance
```

## XGBoost 

```{r, message = F}
# xgboost model development
# optimized with logloss

xg_mod <- xgboost(train_data %>% select(-out) %>% as.matrix(), train_data$out %>% as.numeric() -1, 
               max.depth = 2, eta = 0.3, nround = 50, objective = "binary:logistic", verbose = F)
```

```{r, fig.height = 4, fig.width = 6}
# treeSHAP
bg_x <- train_data[sample(nrow(train_data), 5000), ]
unified_model <- xgboost.unify(xg_mod, data = bg_x)

sv_tree <- treeshap(unified_model, train_data %>% select(-out), verbose = FALSE)
tshap = shapviz(sv_tree)

sv_importance(tshap, kind = "bee") + theme_minimal()
```

## Logistic Regression 
```{r, message = F, warning = F, comment = "   "}
# fit model 
lr_mod <- glm(out ~., family = "binomial", data = train_data)
summary(lr_mod)
```

```{r, message = F, warning = F, comment = "   ", results = "hide"}
# kernel shap 
features <- colnames(train_data %>% select(-out))
x_train  <- train_data %>% select(-out)
bg_x <- train_data[sample(nrow(train_data), 500), ]

shap <- kernelshap(lr_mod, x_train, bg_X = bg_x)
plot <- shapviz(shap)
```

```{r, fig.height = 4, fig.width = 6}
# visualize results
sv_importance(plot, kind = "bee") + theme_minimal()
```

## Model Validation 

```{r, warning = F, message = F }
# Internal validation: assess model performance using the model development data set 

obs       <- train_data$out
pred_xg   <- predict(xg_mod, newdata = train_data %>% select(-out) %>% as.matrix)
pred_lr   <- predict(lr_mod, newdata = train_data %>% select(-out), type = "response")

int_valid <- cbind(obs, pred_xg, pred_lr) %>% as.data.frame() %>% mutate(obs = obs %>% as.numeric()-1)

# assess discrimination 
auc_xg <- as.numeric(pROC::roc(int_valid$obs ~ int_valid$pred_xg, quiet = TRUE)$auc)
auc_lr <- as.numeric(pROC::roc(int_valid$obs ~ int_valid$pred_lr, quiet = TRUE)$auc)

# assess calibration
cali_xg <- 
  int_valid %>% 
    ggplot() + 
    geom_abline(slope = 1, intercept = 0, linewidth = 1, color = "darkgray") +
    geom_smooth(
      aes(x = pred_xg, y = obs), 
      method = "loess", 
      formula = "y ~ x", 
      color = "wheat",
      se = F
      ) +   
    theme_minimal()+ 
    xlim(0,1) + 
    ylim(0,1) + 
    xlab("Predicted Risk") + 
    ylab("Observed Proportion") + 
    ggtitle("XGBoost", subtitle= paste0("AUROC: ", round(auc_xg, digits = 3)))

cali_lr <- 
  int_valid %>% 
    ggplot() + 
    geom_abline(slope = 1, intercept = 0, linewidth = 1, color = "darkgray") +
    geom_smooth(
      aes(x = pred_lr, y = obs), 
      method = "loess", 
      formula = "y ~ x", 
      color = "orange", 
      se = F
    ) + 
    theme_minimal()+ 
    xlim(0,1) + 
    ylim(0,1) + 
    xlab("Predicted Risk") + 
    ylab("Observed Proportion") +
    ggtitle("Logistic Regression", subtitle = paste0("AUROC: ", round(auc_lr, digits = 3)))

library(patchwork)

(cali_xg + cali_lr) + plot_annotation(title = "Internal Validation: Flexible Calibration Curves and AUROC")
```


```{r, warning = F, message = F}
# External validation: assess model performance using a large independent validation data set 

obs       <- valid_data$out
pred_xg   <- predict(xg_mod, newdata = valid_data %>% select(-out) %>% as.matrix)
pred_lr   <- predict(lr_mod, newdata = valid_data %>% select(-out), type = "response")

ext_valid <- cbind(obs, pred_xg, pred_lr) %>% as.data.frame() %>% mutate(obs = obs %>% as.numeric()-1)

# assess discrimination 
auc_xg <- as.numeric(pROC::roc(ext_valid$obs ~ ext_valid$pred_xg, quiet = TRUE)$auc)
auc_lr <- as.numeric(pROC::roc(ext_valid$obs ~ ext_valid$pred_lr, quiet = TRUE)$auc)

# assess calibration 
# assess calibration
cali_xg <- 
  ext_valid %>% 
    ggplot() + 
    geom_abline(slope = 1, intercept = 0, linewidth = 1, color = "darkgray") +
    geom_smooth(
      aes(x = pred_xg, y = obs), 
      method = "loess", 
      formula = "y ~ x", 
      color = "wheat",
      se = F
      ) +   
    theme_minimal()+ 
    xlim(0,1) + 
    ylim(0,1) + 
    xlab("Predicted Risk") + 
    ylab("Observed Proportion") + 
    ggtitle("XGBoost", subtitle= paste0("AUROC: ", round(auc_xg, digits = 3)))

cali_lr <- 
  ext_valid %>% 
    ggplot() + 
    geom_abline(slope = 1, intercept = 0, linewidth = 1, color = "darkgray") +
    geom_smooth(
      aes(x = pred_lr, y = obs), 
      method = "loess", 
      formula = "y ~ x", 
      color = "orange", 
      se = F
    ) + 
    theme_minimal()+ 
    xlim(0,1) + 
    ylim(0,1) + 
    xlab("Predicted Risk") + 
    ylab("Observed Proportion") +
    ggtitle("Logistic Regression", subtitle = paste0("AUROC: ", round(auc_lr, digits = 3)))

library(patchwork)

(cali_xg + cali_lr) + plot_annotation(title = "External Validation: Flexible Calibration Curves and AUROC")
```


